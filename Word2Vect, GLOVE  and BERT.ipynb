{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b23f24",
   "metadata": {},
   "source": [
    "# Text Data - Preprocessing and Text to Numerical Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c41cb",
   "metadata": {},
   "source": [
    "# Text Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2394faac",
   "metadata": {},
   "source": [
    "1.Tokenisation\n",
    "\n",
    "2.Removing special characters\n",
    "\n",
    "3.Convert sentence into lower case\n",
    "\n",
    "4.Removing stop words\n",
    "\n",
    "5.Stemming or Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783d43b1",
   "metadata": {},
   "source": [
    "# Techniques to convert Text to Numerical Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c103a",
   "metadata": {},
   "source": [
    "1.Bag of Words\n",
    "\n",
    "2.TF IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "3.Word2Vec (by Google)\n",
    "\n",
    "4.GloVe (Global Vectors by Stanford) - Not Covered in this notebook\n",
    "\n",
    "5.Pretrained GloVe Embeddings\n",
    "\n",
    "6.FastText (by Facebook) - Not Covered in this notebook\n",
    "\n",
    "7.ELMo (Embeddings from Language Models) - Not Covered in this notebook\n",
    "\n",
    "8.BERT (Bidirectional Encoder Representations from Transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab267284",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8d0ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc94bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text\n",
       "0     it Was the best oF Times $\n",
       "1     It was The worst of times.\n",
       "2     IT 9 was tHe age Of wisdom\n",
       "3  it was thE age of foolishness"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_text = ['it Was the best oF Times $', \n",
    "            'It was The worst of times.',\n",
    "            'IT 9 was tHe age Of wisdom', \n",
    "            'it was thE age of foolishness']\n",
    "\n",
    "df = pd.DataFrame({'text': lst_text})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5701ee07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: joblib in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaa7e237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SNEGHAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SNEGHAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\SNEGHAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Downloading wordnet before applying Lemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a51abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e3f6f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialise the inbuilt Stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2af89663",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also use Lemmatizer instead of Stemmer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84867c8",
   "metadata": {},
   "source": [
    "# Text Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c15f3cf",
   "metadata": {},
   "source": [
    "Text Preprocessing steps include some essential tasks to clean and remove the noise from the available data.\n",
    "\n",
    "1.Removing Special Characters and Punctuation\n",
    "\n",
    "2.Converting to Lower Case - We convert the whole text corpus to lower case to reduce the size of the vocabulary of our text data.\n",
    "\n",
    "3.Removing Stop Words - Stopwords don't contribute to the meaning of a sentence. So, we can safely remove them without changing the meaning of the sentence. For eg: it, was, any, then, a, is, by, etc are the stopwords.\n",
    "\n",
    "4.Stemming or Lemmatization - Stemming is the process of getting the root form of a word. For eg: warm, warmer, warming can be converted to warm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e114b518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 1is Natural-LAnguage-Processing.\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"This 1is Natural-LAnguage-Processing.\"\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97ceb4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This  is Natural LAnguage Processing \n"
     ]
    }
   ],
   "source": [
    "# Removing special characters and digits\n",
    "sentence = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed787024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this  is natural language processing \n"
     ]
    }
   ],
   "source": [
    "# change sentence to lower case\n",
    "sentence = sentence.lower()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7877100a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# tokenize into words\n",
    "tokens = sentence.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d09b04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# Removing stop words\n",
    "clean_tokens = [t for t in tokens if t not in stopwords.words(\"english\")]\n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76267d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natur', 'languag', 'process']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "clean_tokens_stem = [stemmer.stem(word) for word in clean_tokens]\n",
    "print(clean_tokens_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65450a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing\n",
    "clean_tokens_lem = [lemmatizer.lemmatize(word) for word in clean_tokens]\n",
    "print(clean_tokens_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c6fd8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_text, flag):\n",
    "    # Removing special characters and digits\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    \n",
    "    # change sentence to lower case\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    # remove stop words                \n",
    "    clean_tokens = [t for t in tokens if t not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Stemming/Lemmatization\n",
    "    if(flag == 'stem'):\n",
    "        clean_tokens = [stemmer.stem(word) for word in clean_tokens]\n",
    "    else:\n",
    "        clean_tokens = [lemmatizer.lemmatize(word) for word in clean_tokens]\n",
    "    \n",
    "    return pd.Series([\" \".join(clean_tokens), len(clean_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f131d657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0  1\n",
       "0    best time  2\n",
       "1   worst time  2\n",
       "2   age wisdom  2\n",
       "3  age foolish  2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = df['text'].apply(lambda x : preprocess(x, 'stem'))\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b9eef45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clean_text_stem  text_length_stem\n",
       "0       best time                 2\n",
       "1      worst time                 2\n",
       "2      age wisdom                 2\n",
       "3     age foolish                 2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.columns = ['clean_text_stem', 'text_length_stem']\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcbccb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem\n",
       "0     it Was the best oF Times $       best time                 2\n",
       "1     It was The worst of times.      worst time                 2\n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2\n",
       "3  it was thE age of foolishness     age foolish                 2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62a0f5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0  1\n",
       "0        best time  2\n",
       "1       worst time  2\n",
       "2       age wisdom  2\n",
       "3  age foolishness  2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = df['text'].apply(lambda x: preprocess(x, 'lemma'))\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdd8bce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clean_text_lemma  text_length_lemma\n",
       "0        best time                  2\n",
       "1       worst time                  2\n",
       "2       age wisdom                  2\n",
       "3  age foolishness                  2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.columns = ['clean_text_lemma', 'text_length_lemma']\n",
    "\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5217e7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma  \n",
       "0        best time                  2  \n",
       "1       worst time                  2  \n",
       "2       age wisdom                  2  \n",
       "3  age foolishness                  2  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df, temp_df], axis=1)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd8731",
   "metadata": {},
   "source": [
    "# Bag of Word "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a100721",
   "metadata": {},
   "source": [
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81b65936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma  \n",
       "0        best time                  2  \n",
       "1       worst time                  2  \n",
       "2       age wisdom                  2  \n",
       "3  age foolishness                  2  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b38975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "vocab = CountVectorizer()\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "\n",
    "dtm = vocab.fit_transform(df['clean_text_lemma'])\n",
    "\n",
    "# fit_transform() could be done seperatly as mentioned below\n",
    "# vocab.fit(df.clean_text_stem)\n",
    "# dtm = vocab.transform(df.clean_text_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6da5f17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best': 1, 'time': 3, 'worst': 5, 'age': 0, 'wisdom': 4, 'foolishness': 2}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can look at unique words by using 'vocabulary_'\n",
    "\n",
    "vocab.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b626271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Observe that the type of dtm is sparse\n",
    "\n",
    "print(type(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de6c22ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 6)\n"
     ]
    }
   ],
   "source": [
    "# Lets now print the  shape of this dtm\n",
    "\n",
    "print(dtm.shape)\n",
    "\n",
    "# o/p -> (4, 6)\n",
    "# i.e -> 4 documents and 6 unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "528709e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the dtm\n",
    "\n",
    "print(dtm)\n",
    "\n",
    "# Remember that dtm is a sparse matrix. i.e. zeros wont be stored\n",
    "# Lets understand First line of output -> (0,1)    1\n",
    "# Here (0, 1) means 0th document and 1st(index starting from 0) unique word. \n",
    "# (we have total 4 documents) & (we have total 6 unique words)\n",
    "# (0, 1)    1 -> 1 here refers to the number of occurence of 1st word\n",
    "# Now lets read it all in english.\n",
    "# (0, 1)    1 -> 'times' occurs 1 time in 0th document. \n",
    "# Try to observe -> (3, 2)   1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f8aa2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 0]\n",
      " [0 0 0 1 0 1]\n",
      " [1 0 0 0 1 0]\n",
      " [1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Since the dtm is sparse, lets convert it into numpy array.\n",
    "\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b15835f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'best', 'foolishness', 'time', 'wisdom', 'worst']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3c814fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  best  foolishness  time  wisdom  worst\n",
       "0    0     1            0     1       0      0\n",
       "1    0     0            0     1       0      1\n",
       "2    1     0            0     0       1      0\n",
       "3    1     0            1     0       0      0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=sorted(vocab.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69947d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-grams\n",
    "\n",
    "vocab = CountVectorizer(ngram_range=[1,2])\n",
    "\n",
    "dtm = vocab.fit_transform(df.clean_text_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0b4ce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best': 3, 'time': 6, 'best time': 4, 'worst': 8, 'worst time': 9, 'age': 0, 'wisdom': 7, 'age wisdom': 2, 'foolish': 5, 'age foolish': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38f7a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 1]\n",
      " [1 0 1 0 0 0 0 1 0 0]\n",
      " [1 1 0 0 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# convert sparse matrix to numpy array\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75b5c79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age foolish</th>\n",
       "      <th>age wisdom</th>\n",
       "      <th>best</th>\n",
       "      <th>best time</th>\n",
       "      <th>foolish</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "      <th>worst time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  age foolish  age wisdom  best  best time  foolish  time  wisdom  \\\n",
       "0    0            0           0     1          1        0     1       0   \n",
       "1    0            0           0     0          0        0     1       0   \n",
       "2    1            0           1     0          0        0     0       1   \n",
       "3    1            1           0     0          0        1     0       0   \n",
       "\n",
       "   worst  worst time  \n",
       "0      0           0  \n",
       "1      1           1  \n",
       "2      0           0  \n",
       "3      0           0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=sorted(vocab.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5055d69",
   "metadata": {},
   "source": [
    "# Term Frequency Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4427c5e",
   "metadata": {},
   "source": [
    "In BOW approach all the words in the text are treated as equally important i.e. there's no notion of some words in the document being more important than others. TF-IDF, or term frequency-inverse document frequency, addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808101f6",
   "metadata": {},
   "source": [
    "1.Term Frequency\n",
    "\n",
    "2.Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff290bc",
   "metadata": {},
   "source": [
    "TF = Probabilty of Occurence of Word in Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d266c",
   "metadata": {},
   "source": [
    "IDF = Probablity of Occurence of Word in Corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c662a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "dtm = vectorizer.fit_transform(df.clean_text_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5cbb47a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best': 1, 'time': 3, 'worst': 5, 'age': 0, 'wisdom': 4, 'foolishness': 2}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f37381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.78528828 0.         0.6191303  0.         0.        ]\n",
      " [0.         0.         0.         0.6191303  0.         0.78528828]\n",
      " [0.6191303  0.         0.         0.         0.78528828 0.        ]\n",
      " [0.6191303  0.         0.78528828 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(dtm.toarray()) \n",
    "\n",
    "# convert sparse matrix to nparray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "983068f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>best</th>\n",
       "      <th>foolishness</th>\n",
       "      <th>time</th>\n",
       "      <th>wisdom</th>\n",
       "      <th>worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.61913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.785288</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age      best  foolishness     time    wisdom     worst\n",
       "0  0.00000  0.785288     0.000000  0.61913  0.000000  0.000000\n",
       "1  0.00000  0.000000     0.000000  0.61913  0.000000  0.785288\n",
       "2  0.61913  0.000000     0.000000  0.00000  0.785288  0.000000\n",
       "3  0.61913  0.000000     0.785288  0.00000  0.000000  0.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm.toarray(), columns=sorted(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318df9a",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2743fbc3",
   "metadata": {},
   "source": [
    "# Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a0a80",
   "metadata": {},
   "source": [
    "A latent space, also known as a latent feature space or embedding space, is an embedding of a set of items within a manifold in which items which resemble each other more closely are positioned closer to one another in the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6224d52",
   "metadata": {},
   "source": [
    "# Word Embeddings (Word Vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b0866",
   "metadata": {},
   "source": [
    "In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers.\n",
    "\n",
    "Methods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, explainable knowledge base method, and explicit representation in terms of the context in which words appear.\n",
    "\n",
    "Traditionally, one of the main limitations of word embeddings (word vector space models in general) is that words with multiple meanings are conflated into a single representation (a single vector in the semantic space). In other words, polysemy and homonymy are not handled properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728a596",
   "metadata": {},
   "source": [
    "Algorithms to generate Word2Vec Embeddings\n",
    "\n",
    "1.SkipGram\n",
    "\n",
    "2.Continuous Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ce21d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.22.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (6.2.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gensim) (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2316d52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4bf1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "755de24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>[best, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>[worst, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, wisdom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, foolish]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma tokenised_sentences  \n",
       "0        best time                  2        [best, time]  \n",
       "1       worst time                  2       [worst, time]  \n",
       "2       age wisdom                  2       [age, wisdom]  \n",
       "3  age foolishness                  2      [age, foolish]  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenised_sentences'] = df.clean_text_stem.apply(lambda sent : sent.split())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "918ae6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['best', 'time'], ['worst', 'time'], ['age', 'wisdom'], ['age', 'foolish']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.tokenised_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "375fddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "\n",
    "model = Word2Vec(list(df.tokenised_sentences), vector_size=100, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "58aa55fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=6, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc54a632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Documents\n",
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "209af2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 0, 'time': 1, 'foolish': 2, 'wisdom': 3, 'worst': 4, 'best': 5}\n",
      "['age', 'time', 'foolish', 'wisdom', 'worst', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Looking at the vocabulary\n",
    "\n",
    "print(model.wv.key_to_index)\n",
    "\n",
    "print(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0bb4a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# access the 100 dimensional vector for one of the words\n",
    "\n",
    "print(model.wv.__getitem__('foolish'))\n",
    "\n",
    "print(model.wv.__getitem__('foolish').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "900054db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      "  -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      "  -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      "  -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "   2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "   7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "   6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      "  -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "   9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "   8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      "  -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      "  -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "   4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      "  -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "   4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      "  -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      "  -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      "  -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      "  -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "   7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      "  -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      "  -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      "  -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "   3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      "  -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
      " [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
      "   7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
      "  -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
      "  -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
      "   6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
      "   2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
      "   6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
      "  -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
      "  -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
      "   6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
      "   7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
      "  -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
      "   1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
      "  -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
      "   9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
      "  -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
      "   3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
      "   7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
      "   5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
      "  -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      "  -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "   1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
      "   2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
      "   2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
      "   5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
      " [ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "   7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      "  -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "   9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "   5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      "  -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "   7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      "  -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      "  -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      "  -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      "  -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "   3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      "  -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      "  -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      "  -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "   4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      "  -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      "  -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "   4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "   9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "   1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "   1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "   8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "   9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "   5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n",
      " [-8.2426779e-03  9.2993546e-03 -1.9766092e-04 -1.9672764e-03\n",
      "   4.6036304e-03 -4.0953159e-03  2.7431143e-03  6.9399667e-03\n",
      "   6.0654259e-03 -7.5107943e-03  9.3823504e-03  4.6718083e-03\n",
      "   3.9661205e-03 -6.2435055e-03  8.4599797e-03 -2.1501649e-03\n",
      "   8.8251876e-03 -5.3620026e-03 -8.1294188e-03  6.8245591e-03\n",
      "   1.6711927e-03 -2.1985089e-03  9.5136007e-03  9.4938548e-03\n",
      "  -9.7740470e-03  2.5052286e-03  6.1566923e-03  3.8724565e-03\n",
      "   2.0227872e-03  4.3050171e-04  6.7363144e-04 -3.8206363e-03\n",
      "  -7.1402504e-03 -2.0888723e-03  3.9238976e-03  8.8186832e-03\n",
      "   9.2591504e-03 -5.9759365e-03 -9.4026709e-03  9.7643770e-03\n",
      "   3.4297847e-03  5.1661171e-03  6.2823449e-03 -2.8042626e-03\n",
      "   7.3227035e-03  2.8302716e-03  2.8710044e-03 -2.3803699e-03\n",
      "  -3.1282497e-03 -2.3701417e-03  4.2764368e-03  7.6057913e-05\n",
      "  -9.5842788e-03 -9.6655441e-03 -6.1481940e-03 -1.2856961e-04\n",
      "   1.9974159e-03  9.4319675e-03  5.5843508e-03 -4.2906962e-03\n",
      "   2.7831673e-04  4.9643586e-03  7.6983096e-03 -1.1442233e-03\n",
      "   4.3234206e-03 -5.8143795e-03 -8.0419064e-04  8.1000505e-03\n",
      "  -2.3600650e-03 -9.6634552e-03  5.7792603e-03 -3.9298222e-03\n",
      "  -1.2228728e-03  9.9805174e-03 -2.2563506e-03 -4.7570644e-03\n",
      "  -5.3293873e-03  6.9808899e-03 -5.7088719e-03  2.1136629e-03\n",
      "  -5.2556600e-03  6.1207139e-03  4.3573068e-03  2.6063549e-03\n",
      "  -1.4910829e-03 -2.7460635e-03  8.9929365e-03  5.2157748e-03\n",
      "  -2.1625196e-03 -9.4703101e-03 -7.4260519e-03 -1.0637414e-03\n",
      "  -7.9494715e-04 -2.5629092e-03  9.6827205e-03 -4.5852066e-04\n",
      "   5.8737611e-03 -7.4475873e-03 -2.5060738e-03 -5.5498634e-03]\n",
      " [-7.1390150e-03  1.2410306e-03 -7.1767163e-03 -2.2446180e-03\n",
      "   3.7193035e-03  5.8331238e-03  1.1981833e-03  2.1027315e-03\n",
      "  -4.1103913e-03  7.2253332e-03 -6.3070417e-03  4.6472158e-03\n",
      "  -8.2199732e-03  2.0364679e-03 -4.9770521e-03 -4.2476882e-03\n",
      "  -3.1089843e-03  5.6552086e-03  5.7984008e-03 -4.9746488e-03\n",
      "   7.7333092e-04 -8.4957778e-03  7.8098057e-03  9.2572914e-03\n",
      "  -2.7423275e-03  8.0022332e-04  7.4665190e-04  5.4778848e-03\n",
      "  -8.6060790e-03  5.8445573e-04  6.8694223e-03  2.2315944e-03\n",
      "   1.1246764e-03 -9.3221553e-03  8.4823668e-03 -6.2641273e-03\n",
      "  -2.9923737e-03  3.4937870e-03 -7.7262759e-04  1.4112913e-03\n",
      "   1.7819917e-03 -6.8288995e-03 -9.7248117e-03  9.0405848e-03\n",
      "   6.1980546e-03 -6.9129276e-03  3.4034825e-03  2.0606398e-04\n",
      "   4.7537456e-03 -7.1199429e-03  4.0269541e-03  4.3474343e-03\n",
      "   9.9573694e-03 -4.4737398e-03 -1.3892639e-03 -7.3173214e-03\n",
      "  -9.6978294e-03 -9.0802573e-03 -1.0227549e-03 -6.5032900e-03\n",
      "   4.8497282e-03 -6.1640264e-03  2.5191857e-03  7.3944090e-04\n",
      "  -3.3921539e-03 -9.7922329e-04  9.9791251e-03  9.1458866e-03\n",
      "  -4.4618296e-03  9.0830270e-03 -5.6417631e-03  5.9309220e-03\n",
      "  -3.0972182e-03  3.4317516e-03  3.0172265e-03  6.9004609e-03\n",
      "  -2.3738837e-03  8.7750368e-03  7.5894282e-03 -9.5476462e-03\n",
      "  -8.0082091e-03 -7.6378966e-03  2.9232574e-03 -2.7947223e-03\n",
      "  -6.9295205e-03 -8.1282640e-03  8.3091799e-03  1.9904887e-03\n",
      "  -9.3280170e-03 -4.7927164e-03  3.1367384e-03 -4.7132061e-03\n",
      "   5.2808430e-03 -4.2334413e-03  2.6417959e-03 -8.0456873e-03\n",
      "   6.2098862e-03  4.8188888e-03  7.8719261e-04  3.0134476e-03]\n",
      " [-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
      "  -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      "  -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
      "  -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
      "  -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
      "  -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      "  -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
      "   1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
      "  -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
      "   5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
      "   9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
      "  -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
      "  -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
      "   7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
      "   9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
      "   8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "   3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
      "  -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
      "   7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
      "  -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
      "   7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
      "  -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
      "  -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
      "  -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
      "  -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Access the 100D vectors for all 6 words\n",
    "\n",
    "print(model.wv.__getitem__(model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "576e6508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 100)\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.__getitem__(model.wv.index_to_key).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ce37c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = model.wv.__getitem__(model.wv.index_to_key)\n",
    "pca = PCA(n_components = 2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.index_to_key)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5515b102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13887982"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('best', 'worst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ebce33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', 0.17018885910511017),\n",
       " ('best', 0.06408978998661041),\n",
       " ('wisdom', -0.013514922931790352),\n",
       " ('time', -0.023671654984354973),\n",
       " ('age', -0.05234673619270325)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('foolish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "456f2f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>[best, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>[worst, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, wisdom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, foolish]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma tokenised_sentences  \n",
       "0        best time                  2        [best, time]  \n",
       "1       worst time                  2       [worst, time]  \n",
       "2       age wisdom                  2       [age, wisdom]  \n",
       "3  age foolishness                  2      [age, foolish]  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c7fe7",
   "metadata": {},
   "source": [
    "# Sentence Embedding (Document Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe6f84f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best', 'time']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove out-of-vocabulary words\n",
    "\n",
    "sentence = ['best', 'bansal', 'time', 'kanav']\n",
    "\n",
    "vocab_tokens = [word for word in sentence if word in model.wv.index_to_key]\n",
    "\n",
    "vocab_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4d26c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.6735850e-03,  2.8979499e-03,  2.1581696e-03, -1.7885750e-03,\n",
       "       -9.8061212e-04, -3.7891967e-03,  2.7690111e-03,  4.8756767e-03,\n",
       "       -4.6693720e-03, -6.5232953e-03, -2.7048176e-03, -5.3278962e-03,\n",
       "       -6.4251497e-03, -1.2493895e-03,  3.0445517e-04, -5.6858570e-04,\n",
       "        3.8068579e-04,  9.2990650e-04, -3.0666459e-03, -1.1344015e-03,\n",
       "       -3.3043111e-03, -2.6271159e-03,  8.2706194e-03, -1.0838672e-03,\n",
       "       -2.2073742e-04, -3.7620717e-04, -9.0713974e-04, -2.5862674e-03,\n",
       "       -1.3156777e-04,  6.6179251e-03,  7.8556351e-03, -6.5627526e-03,\n",
       "       -2.5582409e-03, -6.9178990e-03,  1.9483893e-03,  6.0251304e-03,\n",
       "        6.4321910e-03,  5.5842018e-03,  7.2997799e-03,  3.0152500e-03,\n",
       "        8.7251253e-03, -7.1729645e-03, -8.2131261e-03, -1.3105709e-03,\n",
       "       -1.9392008e-03,  2.3391065e-03,  2.6729943e-03,  2.9715800e-03,\n",
       "        4.0672242e-04,  8.2550046e-05,  5.2809850e-03, -8.9346431e-03,\n",
       "        3.8251362e-03,  6.0026506e-03, -5.2615297e-03,  5.4140193e-03,\n",
       "        9.4578769e-03, -5.6464854e-04, -3.8392697e-03, -1.3056444e-05,\n",
       "       -6.9205649e-05,  1.2628853e-03,  2.2532057e-03, -6.6235880e-03,\n",
       "        3.6663515e-03,  4.0026912e-03,  4.2561139e-03, -1.7194152e-03,\n",
       "       -1.7866492e-05, -1.3664386e-03, -6.7761191e-04, -1.2985931e-03,\n",
       "        6.7115780e-03,  6.1228252e-03, -7.8244333e-04,  7.9815416e-04,\n",
       "       -8.2111200e-03,  4.5453617e-04,  2.7050036e-03,  9.6003409e-05,\n",
       "       -9.9213817e-04, -1.4492136e-03,  5.2344799e-03, -7.9753939e-03,\n",
       "        2.0204962e-04, -5.3818068e-03, -2.5371348e-03, -2.9714126e-04,\n",
       "        4.1687128e-04, -9.1784005e-04,  5.0794631e-03,  3.6809542e-03,\n",
       "       -3.0058809e-03, -5.4469449e-03,  3.5847109e-03,  3.8838149e-03,\n",
       "        1.8792974e-03, -5.5417679e-03, -3.2742890e-03, -1.0850094e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create document vectors by averaging word vectors\n",
    "\n",
    "np.mean(model.wv.__getitem__(vocab_tokens), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c7996f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(model.wv.__getitem__(vocab_tokens), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "79e65933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc, keyed_vectors):\n",
    "    \"\"\"Remove out-of-vocabulary words. Create document vectors by averaging word vectors.\"\"\"\n",
    "    vocab_tokens = [word for word in doc if word in keyed_vectors.index_to_key]\n",
    "    return np.mean(keyed_vectors.__getitem__(vocab_tokens), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "612bde9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "      <th>doc_vector_w2v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>[best, time]</td>\n",
       "      <td>[-0.008673585, 0.00289795, 0.0021581696, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>[worst, time]</td>\n",
       "      <td>[-0.007879351, 0.0024533845, -0.0009934164, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, wisdom]</td>\n",
       "      <td>[-0.0043894527, 0.004767893, 0.0024528443, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, foolish]</td>\n",
       "      <td>[-0.00022083164, 0.0016568756, -0.0008546477, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma tokenised_sentences  \\\n",
       "0        best time                  2        [best, time]   \n",
       "1       worst time                  2       [worst, time]   \n",
       "2       age wisdom                  2       [age, wisdom]   \n",
       "3  age foolishness                  2      [age, foolish]   \n",
       "\n",
       "                                      doc_vector_w2v  \n",
       "0  [-0.008673585, 0.00289795, 0.0021581696, -0.00...  \n",
       "1  [-0.007879351, 0.0024533845, -0.0009934164, 0....  \n",
       "2  [-0.0043894527, 0.004767893, 0.0024528443, 0.0...  \n",
       "3  [-0.00022083164, 0.0016568756, -0.0008546477, ...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['doc_vector_w2v'] = df.tokenised_sentences.apply(lambda x : document_vector(x, model.wv))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3de04",
   "metadata": {},
   "source": [
    "# Pretrained GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9af3b7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2.0\n",
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(gensim.__version__)\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "186d97ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
     ]
    }
   ],
   "source": [
    "wv = api.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a58efeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2a991721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1193514\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary Size and Word Embedding Shape\n",
    "\n",
    "print(len(wv.index_to_key))\n",
    "\n",
    "print(wv.__getitem__(\"school\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d2b4b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sharepoint', 0.8866323828697205),\n",
       " ('administrator', 0.8673086166381836),\n",
       " ('programmer', 0.8619340658187866),\n",
       " ('architect', 0.8591458201408386),\n",
       " ('oracle', 0.8557209372520447)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"developer\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "235ecf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7817412"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity(\"developer\", \"development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c86525e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.7885  , -0.067292,  0.46616 , -0.81783 ,  0.10752 ,  0.30621 ,\n",
       "        1.4632  , -0.12453 , -0.10154 , -0.23087 , -0.572   ,  0.086826,\n",
       "       -4.051   ,  0.85883 ,  0.71311 , -0.049015, -0.51012 ,  0.22284 ,\n",
       "       -0.98466 ,  0.78809 ,  0.53688 ,  0.17593 ,  0.26659 , -0.86271 ,\n",
       "        0.051508,  0.29894 ,  0.74473 , -0.85046 , -0.32939 , -0.31356 ,\n",
       "        0.63817 , -1.1198  , -0.1482  , -0.46216 ,  0.046157, -0.46282 ,\n",
       "       -0.32383 ,  1.5272  ,  0.76098 , -0.1311  , -0.35028 ,  0.51516 ,\n",
       "       -0.07257 ,  0.2536  ,  0.5363  , -0.46969 ,  0.3285  ,  0.17779 ,\n",
       "       -0.47109 ,  0.37841 ], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.__getitem__('college')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6906e",
   "metadata": {},
   "source": [
    "# Semantic regularities captured in word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c2be4b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8894184"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity(\"college\", \"school\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e7cf98c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81164277"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity(\"college\", \"university\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e81e29fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39391714"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity(\"college\", \"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e410d2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('school', 0.8894184231758118),\n",
       " ('student', 0.8599385023117065),\n",
       " ('class', 0.8441339731216431),\n",
       " ('classes', 0.8319132924079895),\n",
       " ('basketball', 0.8274194002151489)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"college\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ecb28657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yogurt', 0.8689919710159302),\n",
       " ('coconut', 0.8518491387367249),\n",
       " ('strawberry', 0.8414680361747742),\n",
       " ('fanta', 0.8399896621704102),\n",
       " ('mocha', 0.8393071293830872)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(\"mango\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3d908251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('student', 0.8731303811073303),\n",
       " ('students', 0.8377566337585449),\n",
       " ('schools', 0.8342148065567017),\n",
       " ('campus', 0.8218968510627747),\n",
       " ('cambridge', 0.8131343126296997)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['college', 'university'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8de514af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('teacher', 0.8359395265579224),\n",
       " ('form', 0.8157237768173218),\n",
       " ('study', 0.8039187788963318)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(positive=['student', 'class'], negative=['college'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25263756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\n"
     ]
    }
   ],
   "source": [
    "print(wv.doesnt_match(['college', 'university', 'school', 'student', 'apple']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ec0c5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['college', 'mango', 'school','student','class', 'strawberry', 'coconut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5227c249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([wv[word] for word in words])\n",
    "pca = PCA(n_components = 2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e8ef4f",
   "metadata": {},
   "source": [
    "# Sentence Embedding from Pretrained Model (Document Vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab2b0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector_pretrained(doc, keyed_vectors):\n",
    "    \"\"\"Remove out-of-vocabulary words. Create document vectors by averaging word vectors.\"\"\"\n",
    "    vocab_tokens = [word for word in doc if word in keyed_vectors.index_to_key]\n",
    "    return np.mean(keyed_vectors.__getitem__(vocab_tokens), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6d401ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text_stem</th>\n",
       "      <th>text_length_stem</th>\n",
       "      <th>clean_text_lemma</th>\n",
       "      <th>text_length_lemma</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "      <th>doc_vector_w2v</th>\n",
       "      <th>doc_vector_pretrained_glove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it Was the best oF Times $</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>best time</td>\n",
       "      <td>2</td>\n",
       "      <td>[best, time]</td>\n",
       "      <td>[-0.008673585, 0.00289795, 0.0021581696, -0.00...</td>\n",
       "      <td>[0.432825, 0.1588, 0.39231, -0.493835, 0.00431...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was The worst of times.</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>worst time</td>\n",
       "      <td>2</td>\n",
       "      <td>[worst, time]</td>\n",
       "      <td>[-0.007879351, 0.0024533845, -0.0009934164, 0....</td>\n",
       "      <td>[0.73222, 0.23221499, 0.29085773, -0.31792, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IT 9 was tHe age Of wisdom</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>age wisdom</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, wisdom]</td>\n",
       "      <td>[-0.0043894527, 0.004767893, 0.0024528443, 0.0...</td>\n",
       "      <td>[-0.045359, -0.66945, -0.33981, 0.167995, 0.70...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it was thE age of foolishness</td>\n",
       "      <td>age foolish</td>\n",
       "      <td>2</td>\n",
       "      <td>age foolishness</td>\n",
       "      <td>2</td>\n",
       "      <td>[age, foolish]</td>\n",
       "      <td>[-0.00022083164, 0.0016568756, -0.0008546477, ...</td>\n",
       "      <td>[0.027362999, -0.08074499, -0.37674, 0.773955,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text clean_text_stem  text_length_stem  \\\n",
       "0     it Was the best oF Times $       best time                 2   \n",
       "1     It was The worst of times.      worst time                 2   \n",
       "2     IT 9 was tHe age Of wisdom      age wisdom                 2   \n",
       "3  it was thE age of foolishness     age foolish                 2   \n",
       "\n",
       "  clean_text_lemma  text_length_lemma tokenised_sentences  \\\n",
       "0        best time                  2        [best, time]   \n",
       "1       worst time                  2       [worst, time]   \n",
       "2       age wisdom                  2       [age, wisdom]   \n",
       "3  age foolishness                  2      [age, foolish]   \n",
       "\n",
       "                                      doc_vector_w2v  \\\n",
       "0  [-0.008673585, 0.00289795, 0.0021581696, -0.00...   \n",
       "1  [-0.007879351, 0.0024533845, -0.0009934164, 0....   \n",
       "2  [-0.0043894527, 0.004767893, 0.0024528443, 0.0...   \n",
       "3  [-0.00022083164, 0.0016568756, -0.0008546477, ...   \n",
       "\n",
       "                         doc_vector_pretrained_glove  \n",
       "0  [0.432825, 0.1588, 0.39231, -0.493835, 0.00431...  \n",
       "1  [0.73222, 0.23221499, 0.29085773, -0.31792, -0...  \n",
       "2  [-0.045359, -0.66945, -0.33981, 0.167995, 0.70...  \n",
       "3  [0.027362999, -0.08074499, -0.37674, 0.773955,...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['doc_vector_pretrained_glove'] = df.tokenised_sentences.apply(lambda x : document_vector_pretrained(x, wv))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7042c78",
   "metadata": {},
   "source": [
    "# Word2Vec vs BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77df44d9",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ecaaab",
   "metadata": {},
   "source": [
    "Word2Vec offers pre-trained word embeddings that anyone can use off-the-shelf. The embeddings are key: value pairs, essentially 1-1 mappings between words and their respective vectors. Word2Vec takes a single word as input and outputs a single vector representation of that word.\n",
    "\n",
    "Since BERT generates contextual embeddings, it takes as input a sequence (usually a sentence) rather than a single word. BERT needs to be shown the context that surrounding words provide before it can generate a word embedding. With BERT, you do need to have the actual model as the vector representations of words will vary based on the specific sequences you’re inputting. The output is a fixed-length vector representation of the input sentence.\n",
    "\n",
    "BERT or Bidirectional Encoder Representations from Transformers, is a technique that allows for bidirectional training of Transformers for natural language modeling tasks. Language models which are bidirectionally trained can learn deeper context from language than single-direction models. BERT generates context aware embeddings that allow for multiple representations (each representation, in this case, is a vector) of each word based on a given word’s context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c053421c",
   "metadata": {},
   "source": [
    "# Word Ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d3cd0",
   "metadata": {},
   "source": [
    "Word2Vec embeddings do not take into account the word position.\n",
    "\n",
    "BERT model explicitly takes as input the position (index) of each word in the sentence before calculating its embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9869622",
   "metadata": {},
   "source": [
    "# Out-of-Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eccfaea",
   "metadata": {},
   "source": [
    "Since Word2Vec learns embeddings at word level, it can only generate embeddings for words that existed in it’s training set (aka it’s “vocabulary space”). This is a major drawback to Word2Vec - that it just doesn’t support Out-of-Vocabulary words.\n",
    "\n",
    "Alternatively, BERT learns representations at the subword level, so a BERT model will have a smaller vocabulary space than the number of unique words in its training corpus. In turn, BERT is able to generate embeddings for words outside of its vocabulary space giving it a near infinite vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff1ea6",
   "metadata": {},
   "source": [
    "# Sentence BERT (SBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886477ca",
   "metadata": {},
   "source": [
    "Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7ffcf",
   "metadata": {},
   "source": [
    "Usage\n",
    "\n",
    "1.Computing Sentence Embeddings\n",
    "\n",
    "2.Semantic Textual Similarity\n",
    "\n",
    "3.Semantic Search\n",
    "\n",
    "4.Retrieve and Re-Rank\n",
    "\n",
    "5.Clustering\n",
    "\n",
    "6.Paraphrase Mining\n",
    "\n",
    "7.Translated Sentence Mining\n",
    "\n",
    "8.Cross Encoders\n",
    "\n",
    "9.Image Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "628393e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     -------------------------------------- 86.0/86.0 kB 605.7 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
      "     ---------------------------------------- 4.9/4.9 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (4.64.1)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.12.1-cp310-cp310-win_amd64.whl (162.2 MB)\n",
      "     -------------------------------------- 162.2/162.2 MB 1.3 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.13.1-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.22.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (3.7)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp310-cp310-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.6 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "     -------------------------------------- 120.7/120.7 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp310-cp310-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.9.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->sentence-transformers) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision->sentence-transformers) (9.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sneghal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
      "Using legacy 'setup.py install' for sentence-transformers, since package 'wheel' is not installed.\n",
      "Installing collected packages: tokenizers, sentencepiece, torch, torchvision, huggingface-hub, transformers, sentence-transformers\n",
      "  Running setup.py install for sentence-transformers: started\n",
      "  Running setup.py install for sentence-transformers: finished with status 'done'\n",
      "Successfully installed huggingface-hub-0.9.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.22.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "80fe1f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591a5243614746d481839a71efc03bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3b810f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54db13375dd413881f0f04944ee16b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f0610b055b4fceb0156a3d6ecbcc39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df249ba69c5457985aa926ced71e7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eadf8f14b4744a38b9fb90a223148a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c453ae8fa1845f186fb82a4782dd056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f818eaca2aab44de80f3d36fef8f4d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebbaf54c1984bec892b6083a0b5d47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c464be0560d940959130719a83a7e78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246b062645d9456c8e904219d92a5116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c4816951ee4494a79609ba95d032b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb13de1a050549109e82335e1900b1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309832633cfb44409dd89e80ef505657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a1e6fcf6004ae69e777716ada86d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbf5763770042aaa4415ee79d408a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SNEGHAL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:133: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  t = torch.tensor([], dtype=storage.dtype, device=storage._untyped().device)\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
